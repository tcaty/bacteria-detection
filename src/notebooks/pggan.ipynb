{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "pwd = os.getcwd()\n",
    "python_path = pwd[: pwd.rfind(\"/\")]\n",
    "sys.path.append(python_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import lightning\n",
    "import jupyter_black\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from data import GenericDataset\n",
    "from typing import List, Any\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from progress.bar import ChargingBar\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.empty_cache()\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloder(size: int, batch_size: int):\n",
    "    substrates_dataset = GenericDataset(\n",
    "        dir_path=\"/srv/data/raw_support_data\",\n",
    "        transform=transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=0, std=1),\n",
    "                transforms.CenterCrop(size=(800, 1280)),\n",
    "                transforms.Resize((size, size)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "    return DataLoader(dataset=substrates_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(lightning.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int,\n",
    "        batch_size: int = 128,\n",
    "        num_workers: int = 0,\n",
    "        shuffle: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"size\"])\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Grayscale(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=0, std=1),\n",
    "                transforms.CenterCrop(size=(800, 1280)),\n",
    "                transforms.Resize((size, size)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        if stage == \"fit\" or stage is None:\n",
    "            self.train = GenericDataset(\n",
    "                dir_path=\"/srv/data/raw_support_data\",\n",
    "                transform=self.transform,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(self.train, **self.hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pixel_norm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, a: torch.Tensor):\n",
    "        b = a / torch.sqrt(torch.sum(a**2, dim=1, keepdim=True) + 10e-8)\n",
    "        return b\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = 512\n",
    "        self.out_channels = 512\n",
    "        self.model = nn.Sequential(*self._initial_block())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def grow(self, size: int) -> None:\n",
    "        self._decrease_channels(size)\n",
    "        self.model.extend(self._block(self.in_channels, self.out_channels))\n",
    "        if size == 512:\n",
    "            self.model.append(nn.Conv2d(16, 1, kernel_size=1))\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def _decrease_channels(self, size: int):\n",
    "        if size > 16:\n",
    "            self.out_channels //= 2\n",
    "        if size > 32:\n",
    "            self.in_channels //= 2\n",
    "\n",
    "    def _initial_block(self) -> List[Any]:\n",
    "        return [\n",
    "            *self._conv(\n",
    "                in_channels=self.in_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=4,\n",
    "                padding=3,\n",
    "            ),\n",
    "            *self._conv(\n",
    "                in_channels=self.out_channels,\n",
    "                out_channels=self.out_channels,\n",
    "                kernel_size=3,\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def _block(self, in_channels: int, out_channels: int) -> List[Any]:\n",
    "        return [\n",
    "            nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "            *self._conv(in_channels, out_channels),\n",
    "            *self._conv(out_channels, out_channels),\n",
    "        ]\n",
    "\n",
    "    def _conv(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        padding: int = 1,\n",
    "    ) -> List[Any]:\n",
    "        return [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            Pixel_norm(),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = 512\n",
    "        self.out_channels = 512\n",
    "        self.model = nn.Sequential(*self._initial_block())\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.model(x)\n",
    "\n",
    "    def grow(self, size: int) -> None:\n",
    "        self._decrease_channels(size)\n",
    "        self.model = self._block(self.in_channels, self.out_channels) + self.model\n",
    "        if size == 512:\n",
    "            self.model = self._end_block() + self.model\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def _decrease_channels(self, size: int):\n",
    "        if size > 16:\n",
    "            self.in_channels //= 2\n",
    "        if size > 32:\n",
    "            self.out_channels //= 2\n",
    "\n",
    "    def _initial_block(self) -> List[Any]:\n",
    "        return [\n",
    "            *self._conv(self.in_channels, self.out_channels, kernel_size=3),\n",
    "            *self._conv(self.out_channels, self.out_channels, kernel_size=4, stride=3),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 1),\n",
    "        ]\n",
    "\n",
    "    def _block(self, in_channels: int, out_channels: int):\n",
    "        return nn.Sequential(\n",
    "            *self._conv(in_channels, out_channels),\n",
    "            *self._conv(out_channels, out_channels),\n",
    "            nn.AvgPool2d(kernel_size=2),\n",
    "        )\n",
    "\n",
    "    def _end_block(self):\n",
    "        return nn.Sequential(\n",
    "            nn.Sequential(nn.Conv2d(1, 16, kernel_size=1)), nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def _conv(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int = 3,\n",
    "        padding: int = 1,\n",
    "        stride: int = 1,\n",
    "    ) -> List[Any]:\n",
    "        return [\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_in_channels(x: torch.Tensor, channels: int) -> torch.Tensor:\n",
    "    temp = np.array([x.cpu().numpy() for _ in range(channels)])\n",
    "    shape = [*temp.shape]\n",
    "    shape = (shape[1], channels, *shape[-2:])\n",
    "    return torch.tensor(temp.reshape(shape)).to(device)\n",
    "\n",
    "\n",
    "class PGGAN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "        self.D = Discriminator()\n",
    "        self.G = Generator()\n",
    "        self.size = 4\n",
    "\n",
    "    def configure_optimizers(self, D_lr: float, G_lr: float):\n",
    "        D_optim = optim.Adam(self.D.parameters(), lr=D_lr, betas=(0.5, 0.99))\n",
    "        G_optim = optim.Adam(self.G.parameters(), lr=G_lr, betas=(0.5, 0.99))\n",
    "        return [D_optim, G_optim]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.G(x)\n",
    "\n",
    "    def grow(self, size: int) -> None:\n",
    "        self.G.grow(size)\n",
    "        self.D.grow(size)\n",
    "        self.size = size\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        batch: torch.Tensor,\n",
    "        batch_idx: int,\n",
    "        D_optim_frequency: int = 1,\n",
    "        G_optim_frequency: int = 1\n",
    "    ):\n",
    "        x = repeat_in_channels(\n",
    "            batch, self.G.out_channels if self.size < 1024 else 1\n",
    "        ).to(device)\n",
    "        D_optim, G_optim = self.optimizers()  # type: ignore\n",
    "\n",
    "        if batch_idx % D_optim_frequency == 0:\n",
    "            z = torch.rand((len(batch), 512, 1, 1)).to(device)\n",
    "            loss = -(torch.mean(self.D(x)) - torch.mean(self.D(self.G(z))))\n",
    "            self.on_training_step_end(D_optim, loss)\n",
    "\n",
    "        if batch_idx % G_optim_frequency == 0:\n",
    "            z = torch.rand((len(batch), 512, 1, 1)).to(device)\n",
    "            loss = -torch.mean(self.D(self.G(z)))\n",
    "            self.on_training_step_end(G_optim, loss)\n",
    "\n",
    "    def on_training_step_end(self, optim, loss: torch.Tensor) -> None:\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PGGAN is not attached to a `Trainer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m dataloader \u001b[39m=\u001b[39m get_dataloder(hparams\u001b[39m.\u001b[39msize, batch_size\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mbatch_size)\n\u001b[1;32m     26\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m---> 27\u001b[0m     pggan\u001b[39m.\u001b[39;49mtraining_step(batch, i)\n\u001b[1;32m     28\u001b[0m pggan\u001b[39m.\u001b[39mgrow(hparams\u001b[39m.\u001b[39msize)\n",
      "Cell \u001b[0;32mIn[10], line 50\u001b[0m, in \u001b[0;36mPGGAN.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\n\u001b[1;32m     43\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     44\u001b[0m     batch: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     45\u001b[0m     batch_idx: \u001b[39mint\u001b[39m,\n\u001b[1;32m     46\u001b[0m ):\n\u001b[1;32m     47\u001b[0m     x \u001b[39m=\u001b[39m repeat_in_channels(\n\u001b[1;32m     48\u001b[0m         batch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mG\u001b[39m.\u001b[39mout_channels \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m<\u001b[39m \u001b[39m1024\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m     49\u001b[0m     )\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 50\u001b[0m     D_optim, G_optim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizers()  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     log_dict \u001b[39m=\u001b[39m {}\n\u001b[1;32m     54\u001b[0m     D_optim_frequency: \u001b[39mint\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhparams\u001b[39m.\u001b[39mD_optim_frequency  \u001b[39m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py:161\u001b[0m, in \u001b[0;36mLightningModule.optimizers\u001b[0;34m(self, use_pl_optimizer)\u001b[0m\n\u001b[1;32m    159\u001b[0m     opts: MODULE_OPTIMIZERS \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fabric_optimizers\n\u001b[1;32m    160\u001b[0m \u001b[39melif\u001b[39;00m use_pl_optimizer:\n\u001b[0;32m--> 161\u001b[0m     opts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_optimizers\n\u001b[1;32m    162\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     opts \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39moptimizers\n",
      "File \u001b[0;32m/opt/conda/envs/venv/lib/python3.10/site-packages/lightning/pytorch/core/module.py:201\u001b[0m, in \u001b[0;36mLightningModule.trainer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m _TrainerFabricShim(fabric\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fabric)  \u001b[39m# type: ignore[return-value]\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_is_scripting \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is not attached to a `Trainer`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    202\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trainer\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PGGAN is not attached to a `Trainer`."
     ]
    }
   ],
   "source": [
    "class HParams:\n",
    "    def __init__(self, size: int, batch_size: int, epochs: int):\n",
    "        self.size = size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "schedule = [\n",
    "    HParams(size=4, batch_size=16, epochs=1),\n",
    "    HParams(size=8, batch_size=16, epochs=1),\n",
    "    HParams(size=16, batch_size=16, epochs=1),\n",
    "    HParams(size=32, batch_size=16, epochs=1),\n",
    "    HParams(size=64, batch_size=8, epochs=1),\n",
    "    HParams(size=128, batch_size=4, epochs=1),\n",
    "    HParams(size=256, batch_size=2, epochs=1),\n",
    "    HParams(size=512, batch_size=1, epochs=1),\n",
    "    HParams(size=1024, batch_size=1, epochs=1),\n",
    "]\n",
    "\n",
    "logger = TensorBoardLogger(save_dir=\"../../logs/bacterias_pggan\")\n",
    "pggan = PGGAN()\n",
    "pggan.configure_optimizers(D_lr=0.0001, G_lr=0.002)\n",
    "\n",
    "for hparams in schedule:\n",
    "    dataloader = get_dataloder(hparams.size, batch_size=hparams.batch_size)\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        pggan.training_step(batch, i)\n",
    "    pggan.grow(hparams.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m         D_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(torch\u001b[39m.\u001b[39mmean(D(x)) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mmean(D(G(z))))\n\u001b[1;32m     34\u001b[0m         on_train_step_end(D_optim, D_loss)\n\u001b[0;32m---> 35\u001b[0m         G_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39;49mmean(D(G(z)))\n\u001b[1;32m     36\u001b[0m         on_train_step_end(G_optim, G_loss)\n\u001b[1;32m     38\u001b[0m G\u001b[39m.\u001b[39mgrow(size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "SIZES = [4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "EPOCHS = 50\n",
    "LR, BETAS = 0.001, (0.0, 0.99)\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "\n",
    "G_optim = optim.Adam(G.parameters(), lr=LR, betas=BETAS)\n",
    "D_optim = optim.Adam(D.parameters(), lr=LR, betas=BETAS)\n",
    "\n",
    "tbl = TensorBoardLogger(save_dir=\"../../logs/substrates_pggan\")\n",
    "tbl.log_hyperparams(\n",
    "    {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"lr\": LR,\n",
    "        \"b1\": BETAS[0],\n",
    "        \"b2\": BETAS[1],\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "    }\n",
    ")\n",
    "\n",
    "for size in SIZES:\n",
    "    dataloader = get_dataloder(size, BATCH_SIZE)\n",
    "    for epoch in range(EPOCHS):\n",
    "        for _, batch in enumerate(dataloader):\n",
    "            z = torch.rand((len(batch), 512, 1, 1)).to(device)\n",
    "            x = repeat_in_channels(batch, G.out_channels if size < 1024 else 1).to(\n",
    "                device\n",
    "            )\n",
    "\n",
    "            D_loss = -(torch.mean(D(x)) - torch.mean(D(G(z))))\n",
    "            on_train_step_end(D_optim, D_loss)\n",
    "            G_loss = -torch.mean(D(G(z)))\n",
    "            on_train_step_end(G_optim, G_loss)\n",
    "\n",
    "    G.grow(size)\n",
    "    D.grow(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
